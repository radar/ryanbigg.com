<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-05-04T20:27:04+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ryan Bigg’s Blog</title><entry><title type="html">Show, Don’t Tell</title><link href="http://localhost:4000/2025/05/show-dont-tell" rel="alternate" type="text/html" title="Show, Don’t Tell" /><published>2025-05-03T00:00:00+10:00</published><updated>2025-05-03T00:00:00+10:00</updated><id>http://localhost:4000/2025/05/show-dont-tell</id><content type="html" xml:base="http://localhost:4000/2025/05/show-dont-tell"><![CDATA[<p>On Monday night, I’m going to be on a panel in Melbourne, in front of a crowd of aspirational junior developers, answering questions and giving advice. I’ve been <a href="https://ryanbigg.com/2018/03/hiring-juniors">a proponent for junior developers for a very long time</a>, and ran two successful iterations of my Junior Engineering Program at Culture Amp, ending in 2019, as well as continuing to mentor developers in my current line of work.</p>

<p>My advice to the juniors of 2025 is plain and simple: <strong>Show, Don’t Tell.</strong> The first time I hear from a lot of juniors is probably when they apply for a job, or reach out about one. It used to be meetups but the tyranny of distance got in the way.</p>

<p>When they reach out, that’s when I’ll find out the regular things of what tools they’ve used. HTML, CSS, JavaScript, some framework or another. Catch me on a good day (most of them) and I’ll even take a look at their GitHub profiles and portfolios. I’m a curious sort of guy.</p>

<p>The ones that stand out the most do a really great job of <em>showing</em> me that they know the tools, and that they’ve gone past a first tutorial stage.</p>

<ul>
  <li>A React app that ranks your favourite books, then orders them by read date, then reorders them by cover colour.</li>
  <li>A game you made because you had an idea you couldn’t leave behind. Yes, even if the game is naff.</li>
  <li>Show me a thing I didn’t think CSS could do, ever.</li>
</ul>

<p>All of this goes a long way to showing me an aptitude that already puts you ahead of 90% of the competition. These are the outliers I will notice and think more about.</p>

<p>So: <em>Show me</em> what you can do, rather than giving me a list of tools. A Luthier and I both know how to use a saw, but only one of us knows how to make a guitar. The proof is in the doing, not the telling.</p>

<p><small>(And for god sake: use a colour other than black and white on your resumé!)</small></p>]]></content><author><name></name></author><summary type="html"><![CDATA[On Monday night, I’m going to be on a panel in Melbourne, in front of a crowd of aspirational junior developers, answering questions and giving advice. I’ve been a proponent for junior developers for a very long time, and ran two successful iterations of my Junior Engineering Program at Culture Amp, ending in 2019, as well as continuing to mentor developers in my current line of work.]]></summary></entry><entry><title type="html">Cursor-based querying with Rails</title><link href="http://localhost:4000/2025/04/cursor-based-querying" rel="alternate" type="text/html" title="Cursor-based querying with Rails" /><published>2025-04-03T00:00:00+11:00</published><updated>2025-04-03T00:00:00+11:00</updated><id>http://localhost:4000/2025/04/cursor-based-querying</id><content type="html" xml:base="http://localhost:4000/2025/04/cursor-based-querying"><![CDATA[<p>It’s a well known issue that <code>LIMIT</code> + <code>OFFSET</code> pagination in any SQL server will lead to performance problems once the value of <code>OFFSET</code> reaches a high enough value. This is because the database has to scan through the first [<code>OFFSET</code> amount] of records that match the query before it can start returning an amount of records up to the <code>LIMIT</code>.</p>

<p>This sort of addition of a <code>LIMIT</code> + <code>OFFSET</code> to a slow query is commonly also used as a stop-gap for expensive queries. Perhaps before adding this, you have a query that’s building up a long list of transactions for another business to consume, and then one of your customers has a particularly impressive day and then your database has a particularly not-so-impressive time with that query. No problem, you think, you’ll find the data in batches of 1000 by using a <code>LIMIT</code> and <code>OFFSET</code> (such as how <code>find_in_batches</code> in Rails operates). This query will operate <em>better</em> than one without, but as soon as that <code>OFFSET</code> value hits a high number, you’ll run into performance problems again.</p>

<p>When I’ve run into these problems, I’ve turned to the <a href="https://github.com/afair/postgresql_cursor">postgresql_cursor</a> gem. This gem uses <a href="https://www.postgresql.org/docs/current/plpgsql-cursors.html">PostgreSQL cursors</a> to iterate through all the rows of a query without loading the entire query at once.</p>

<p>We can use this in application by calling its methods on a model:</p>

<pre><code class="language-ruby">Purchase.each_instance do |purchase|
  # do something with the data here
end
</code></pre>

<p>This will instantiate each of the rows into instances of the model, but sometimes you just want the raw data instead. For that, the gem provides a different method:</p>

<pre><code class="language-ruby">Purchase.each_row do |row|
  # do something with the raw data
end
</code></pre>

<p>This breaks the queries down by defining a cursor and then iterating through the rows in batches of 1,000 by default. Here’s an example of what the queries for this look like in an application I’m running locally:</p>

<pre><code>   (2.0ms)  declare cursor_58f312c30e9a4719826fbdef24ed2017 no scroll cursor for SELECT "purchases".* FROM "purchases"
   (16.5ms)  fetch 1000 from cursor_58f312c30e9a4719826fbdef24ed2017
   (0.2ms)  fetch 1000 from cursor_58f312c30e9a4719826fbdef24ed2017
   (0.1ms)  close cursor_58f312c30e9a4719826fbdef24ed2017
</code></pre>

<p>Once I’m done working on the first set of thousand, then the gem will fetch the next thousand by calling <code>fetch 1000 from &lt;cursor_id&gt;</code>, with a final call to close off the cursor once there’s no more data returned.</p>

<p>This massively eases the memory pressure on the database as it doesn’t need to load more than 1,000 records at a single time, and keeps its performance linear even if we’re iterating through a whole bunch of different records. All without needing a <code>LIMIT</code> or <code>OFFSET</code>!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[It’s a well known issue that LIMIT + OFFSET pagination in any SQL server will lead to performance problems once the value of OFFSET reaches a high enough value. This is because the database has to scan through the first [OFFSET amount] of records that match the query before it can start returning an amount of records up to the LIMIT.]]></summary></entry><entry><title type="html">Note taking</title><link href="http://localhost:4000/2025/03/note-taking" rel="alternate" type="text/html" title="Note taking" /><published>2025-03-18T00:00:00+11:00</published><updated>2025-03-18T00:00:00+11:00</updated><id>http://localhost:4000/2025/03/note-taking</id><content type="html" xml:base="http://localhost:4000/2025/03/note-taking"><![CDATA[<p>There was a question on the Ruby Oceania Slack recently:</p>

<blockquote>
  <p>What tools/apps are folks using in 2025 to manage their own tasks/life?</p>
</blockquote>

<p>I gave an answer, which I’ve modified slightly for blogability, and kept focussed to just note taking:</p>

<p>Physical A5 note book with 0.8mm Uni-Ball Fineliner in either blue or black depending on the mood. Coincidentally, <a href="https://www.theverge.com/2024/11/25/24305832/sam-altman-pen-notebook-muji-uniball">Sam Altmann has similar tastes.</a></p>

<p>Each page is a day. Write down intentions at start of day and then add to list as day continues. Review calendar, note down meetings and their times. Finish day by reviewing the list from the day and figuring out what to do next, then writing notes into next day’s page if necessary. Good for brain dumping end of day to then clear brain for home.</p>

<p>Bigger projects, longer term storage: <a href="https://bear.app/">Bear app</a>, which is similar in featureset to <a href="https://obsidian.md/">Obsidian</a>. Typically one note per project, person, team or theme. Most of the time these are date-headed as well, so for example project standup today was headed with March 17th, and can tag that with <code>#Journal/2025/03/17</code> so I could look at <code>#Journal/2025/03</code> and find all the things that I thought were notable for the month.</p>

<p>One on one notes with reports or managers go into this app directly as it’s helpful to track certain initiatives or discussions over time. Each time we talk, there’s as sub-heading in the person’s note with the date. All Bear notes being date-headed means I can say with assurance that “you said X on Y date” with some degree of confidence.</p>

<p>Other notes of note:</p>

<ul>
  <li>(passworded) Tax Return check list (find these invoices for all your sass apps, other subscriptions, here’s what you had last year and their costs…)</li>
  <li>Blog post drafts a plenty (including this one)</li>
  <li>A very rough outline of a DND one shot I’m planning</li>
  <li>A list of magic cards I’m seeking</li>
  <li>Local burger shop order</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[There was a question on the Ruby Oceania Slack recently:]]></summary></entry><entry><title type="html">Decorating arrays with SimpleDelegator</title><link href="http://localhost:4000/2025/03/decorating-arrays-with-simpledelegator" rel="alternate" type="text/html" title="Decorating arrays with SimpleDelegator" /><published>2025-03-03T00:00:00+11:00</published><updated>2025-03-03T00:00:00+11:00</updated><id>http://localhost:4000/2025/03/decorating-arrays-with-simpledelegator</id><content type="html" xml:base="http://localhost:4000/2025/03/decorating-arrays-with-simpledelegator"><![CDATA[<p>Let’s say that I have a long list of transactions and that I want to apply some filtering steps to these with Ruby. I might have gathered this list from somewhere in particular, or I could generate it with some quick Ruby:</p>

<pre><code class="language-ruby">Transaction  = Data.define(:date, :amount, :status)

transactions = 100.times.map do
  Transaction.new(
    date: Date.today - rand(30),
    amount: rand(1.0..250.0).round(2),
    status: rand &lt; 0.9 ? "Approved" : "Declined"
  )
end
</code></pre>

<p>These transactions are a list occurring anywhere in the last 30 days, with amounts between $1 and $250, with a status that has a 90% chance of being “Approved” and 10% chance of being “Declined”.</p>

<p>To filter on these I can use common methods like <code>select</code>:</p>

<pre><code class="language-ruby">transactions
  .select { it.amount &lt;= 25 }
  .select { it.date == Date.parse("2025-02-26") }
</code></pre>

<p>That would find me any transaction with an amount less than $25, occurring on the 26th of February. Easy enough!</p>

<p>But we can bring this code closer to English by using <code>SimpleDelegator</code> to decorate our array, creating a neat DSL:</p>

<pre><code class="language-ruby">class Transactions &lt; SimpleDelegator
  def amount_lte(amount)
    select { it.amount &lt;= amount }
  end

  def for_date(date)
    select { it.date == Date.parse(date) }
  end

  def select(&amp;block)
    self.class.new(super(&amp;block))
  end
end
</code></pre>

<p>This class inherits from SimpleDelegator and defines methods to provide that simple DSL. Our code from before:</p>

<pre><code class="language-ruby">transactions
  .select { it.amount &lt;= 25 }
  .select { it.date == Date.parse("2025-02-26") }
</code></pre>

<p>Can now instead be written as:</p>

<pre><code class="language-ruby">transactions = Transactions.new(transactions)
transactions
  .amount_lte(25)
  .for_date("2025-02-06")
</code></pre>

<p>This has centralized the implementation details of how we query the transactions into one simple class. Anything that needs to massage the input before we run a query on transactions now has a nice place to live. An example of this is to put <code>Date.parse</code> inside <code>for_date</code>. This could be customized further to <em>only</em> do that <code>Date.parse</code> if the object passed in is a string and not a <code>Date</code> already.</p>

<p>As a bit of “homework” here, can you update this class to add methods for finding only approved or declined transactions? Is there a chance you could make outside this <code>Transactions</code> class to make the syntax cleaner?</p>

<p>Could you also support this syntax?</p>

<pre><code class="language-ruby">transactions.for_date(date_1).or(transactions.for_date(date_2))
</code></pre>

<p>And now can you write that code any shorter as well?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Let’s say that I have a long list of transactions and that I want to apply some filtering steps to these with Ruby. I might have gathered this list from somewhere in particular, or I could generate it with some quick Ruby:]]></summary></entry><entry><title type="html">Ghosts ‘n’ Stuff</title><link href="http://localhost:4000/2025/02/ghosts-n-stuff" rel="alternate" type="text/html" title="Ghosts ‘n’ Stuff" /><published>2025-02-10T00:00:00+11:00</published><updated>2025-02-10T00:00:00+11:00</updated><id>http://localhost:4000/2025/02/ghosts-n-stuff</id><content type="html" xml:base="http://localhost:4000/2025/02/ghosts-n-stuff"><![CDATA[<p>Being a lead developer is an interesting time. I’d write a lot more blog posts if I wasn’t so busy, sure, but mostly I’d write them if I was <em>allowed to write them</em>. So many times I think “this’d make an interesting blog post” right before the thought of “imagine how much shit you’d be in if you told a soul”. There’s a lot about being a lead that’s <em>interesting</em> but also <em>highly confidential</em>. I’d love to share those stories one day, perhaps a long way down the track.</p>

<p>But today I want to talk about ghosts.</p>

<p>The apps I’m working on have the lucky advantage of being around a decade and a half old. They also have the unlucky disadvantage of being a decade and a half old. Ruby and to a larger extent JavaScript tastes have rapidly evolved in this time. Both have since slowed to a much more agreeable-to-this-dad pace, and I am thankful for that.</p>

<p>Not only do the languages change over time, but the applications do as well. Features get added, seldom removed. Bugs get removed and (hopefully) seldom added. Developers move on — Peopleware claims the average staying power of a dev was in the range of 15-36 months (I would be interested to hear how this has changed after the pandemic) — and new developers come in and claim their way is superior and this situation repeats three to five fold before the current day. The old developers then become “ghosts” of our history.</p>

<p>The application gets into this liminal state of being “complete” (actually a well-known and appreciated fallacy in dev circles, but bear with me), owned by nobody (as those people have left and newer priorities meant the new people haven’t seen this app yet) and yet somehow business critical. I’m talking “people don’t get paid if this doesn’t run”, business critical. The app logs nothing to nowhere, and yet people rely on it intensely and would tell you quite quickly in all-caps if it wasn’t working.</p>

<p>Perhaps this application is deployed to some sort of cloud compute environment and runs as its own function or suite of functions. Can’t have it running in a monolith as a worker because that’s boring and doesn’t add any keywords to your resume for potential recruiters to find – aka Resume Driven Development. How those functions tie together is a corkboard-and-red-string job for the lucky person who finds this app later.</p>

<p>Time comes along and does its thing and the people who run the cloud compute environment say “we’re not going to support that version of that language any more because it’s <em>old</em>”. This announcement is made so far ahead of that deadline that nobody within the business seems to mind.</p>

<p>The future arrives the next day, or it feels that way.</p>

<p>By then, the security auditors come along and say “we require you to keep your systems up to date as possible, and yes, we mean even down to your packages.” And they make the very strong implication that if this isn’t done, that you may as well all go on a big company-wide holiday until it is. On a budget, of course, because if the company isn’t running then nobody’s getting paid a whole lot of anything.</p>

<p>Then the serious discussions about how to approach these upgrades happen. Lots of stern faces. Me remaining mum about my borderline-obsession of banging the drum of “upgrade your apps or you’ll regret it” aka “pay the piper”. People really don’t like to hear “we’re spending time upgrading apps” when instead they could be hearing “we’re spending time delivering business and/or stakeholder value”, aka earning the dollars to keep things afloat. The dollars are nice, sure, but we can’t earn dollars if we’re not compliant with what our auditors require. So the audit wins the priority battle when a deadline is issued.</p>

<p>(Anecdata: on the time split: 70% Value vs 30% maintenance is a good balance that works for my team, we’ve found. We track this mostly by doing what a lot of teams do:
wetting a finger and sticking it in the air. On a good day we might even try to get the data out of our ticketing system.)</p>

<p>That particular box of radioactive applications gets handballed around until it lands onto someone’s lap and then they get to have a “learning and development journey” of upgrading multiple applications of all sorts of flavours and varieties (think: gourmet ice cream shop, but apps), because from about 2014-2018 people decided microservices were the go. It becomes evident very quickly that someone or someone’s were practicing the aforementioned resume driven development.</p>

<p>The task of auditing these applications and upgrading them should fall to a whole team, but more than likely it’ll land in one person’s lap. They can recruit others to help, and they’d be pretty silly not to. It’s always a huge task.</p>

<hr />

<p>The upgrade begins. The mind initially turns to questions like “How bad could it possibly be?”, then “What were these developers on?” and finally arriving at “Why, oh God why?” – a milder version of the Five Stages of Grief – as app after app reveals gradually the eldritch horrors of past coding styles, methodologies and arcane deployment strategies filled with reams of equally dubious and artisanal YAML. (Were these developers being paid by the line?)</p>

<p>(Of course, the way we code <em>today</em> would <strong>never</strong> be viewed with that particular lens. Us being absolute beacons of
perfection having learned so much over our long and storied careers, unpressured by deadlines and unbiased by our current obsessions.)</p>

<p>The archaeological layers of these applications are sometimes stone, and at other times more… biological. You start to learn the quirks and styles of developers and can sight-identify code-strata where <em>this</em> block of code was written by Dev A, but <em>that</em> block of code was Dev B.  And this block of code was disputed territory. Both devs haven’t worked at the company in nearly half a decade. Their names aren’t recognised by most people who are current employees. A search online for them turns up only the fact that they probably existed.</p>

<p>Both devs write good code in parts. You’d tick it in a PR review, or perhaps leave a pedantic comment about nested ternaries being unsightly. You imagine in-person meetings, perhaps at a meetup or a conference, between yourself and these devs, and deciding if during those meetings you want to shake their hand to say “well done” or your head in dismay to say the opposite, depending on what’s in view at the minute.</p>

<p>Dev C, the most recent author in the <code>git log</code> history with a commit measuring in the tens of lines, who happens to still work at the company denies all knowledge of having ever worked on or near this system. Yet the proof is right there in black and white, or other colours depending on your terminal theme du jour.</p>

<p>Any attempt to run these apps is met with things like arcane compilation issues because that one particular gem doesn’t work with the CPU architecture of the machine you’re now using. The newer version of the gem will install just fine… just right after you switch to the minimum language version that it now supports. Occasionally, the gem hasn’t received an update since many years ago.</p>

<p>I initially bristled at this thing in particular: (Sass should be a gem, god damn it! It’s always been a gem!) but in the 3-point-something years of my FE-lead tenure I’ve come around to being a zealot of “CSS and JS are tools for the frontend, and the frontend stack is compiled with <em>these</em> tools” where the tools are usually written in one of the holy trinity of Go, Rust or (to a lesser extent) JavaScript. Or I guess in the case of Sass, Dart. The <code>dartsass-rails</code> gem looks promising, however.</p>

<p>The gems gets upgraded, and you take extra special glee in removing <code>sassc</code> which has a compilation time measured in eons. Rubocop gets told to shove it a few new extra times and some quirks of the code’s setup (such as deprecation warnings) are addressed. Perfection is never achieved not only because it’s extremely subjective but also because time is relentless.</p>

<hr />

<p>These ghosts of the past were doing what they thought was the right thing that was acceptable under the circumstances and conditions that they were working in. Looking at the code now, is it still the right thing? That depends on who you’re asking. Code is subjective. There are arguably the “best” way to do certain things (hello, sorting algorithms) and then there are multitudinous ways of saying “put this JavaScript code on a server somewhere and make it wait for things to happen, so it can tell other things to happen”. That is where things get murky: the points of differences between how they, the ghosts, would do it and how I, the lead developer living in this current time and being tasked with working with this app in the <em>here and now</em>, would do it.</p>

<p>Tastes change. The right way to write and deploy an application is akin to shifting sands. Even this week, the release of Docker’s “bake” command will change how I personally build apps. What was in style five years ago can be, considered a taboo in modern times. The blessed becomes the unspeakable. Developers are a finnicky bunch.</p>

<p>I’ve left unintentional sins in systems I’ve worked on in the past, I’m sure of it. There’s stuff I’ve written that I’m certain of being a net-negative outcome of the apps I work in right now. Unintentional booby traps waiting to catch the next developer who happens across them. There’s other stuff I’m more on the fence about (such as when I introduced and encouraged the use of GraphQL and integrating it with TypeScript — let’s call it 65%/35% love/hate today). And then there’s stuff I’d use as exemplary things during an interview (while sweeping things in Category 1 waaaayyy under the rug).</p>

<p>I think we need to be kind to the ghosts that have left their fingerprints on the systems we work in and on. They, overall, were doing their best. And I also think that we, being the ghosts of the future, need to strive to do our best to leave the <em>best</em> kind of fingerprints we can now. And to be kind to <em>ourselves</em> when the circumstances and conditions mean that we have to be flexible on what “right” means today.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Being a lead developer is an interesting time. I’d write a lot more blog posts if I wasn’t so busy, sure, but mostly I’d write them if I was allowed to write them. So many times I think “this’d make an interesting blog post” right before the thought of “imagine how much shit you’d be in if you told a soul”. There’s a lot about being a lead that’s interesting but also highly confidential. I’d love to share those stories one day, perhaps a long way down the track.]]></summary></entry><entry><title type="html">Rails tagged logging</title><link href="http://localhost:4000/2025/02/rails-tagged-logging" rel="alternate" type="text/html" title="Rails tagged logging" /><published>2025-02-02T00:00:00+11:00</published><updated>2025-02-02T00:00:00+11:00</updated><id>http://localhost:4000/2025/02/rails-tagged-logging</id><content type="html" xml:base="http://localhost:4000/2025/02/rails-tagged-logging"><![CDATA[<p>A feature within Rails is that it allows us to add data to our application’s log lines for tagging those lines. We can then use these tags for aggregating them together into a bunch. I’ll show you how to do this here. I used this in a Rails app that acts purely as an API, so there is only ever one request at a time I care about, in this case.</p>

<p>We can configure this in <code>config/environments/development.rb</code>:</p>

<pre><code>config.log_tags = [lambda {|r| Time.now.iso8601(4) }, :request_id]
</code></pre>

<p>This sets up two tags, one of a timestamp with millisecond precision, and another with the request ID. The symbol <code>:request_id</code> maps to a method on <code>ActionDispatch::Request</code>, and we can use any methods from that class in these tags if we wish.</p>

<p>This log line configuration as it stands now will output this prefix to all log lines:</p>

<pre><code>[2025-02-02T16:32:35.6772+11:00] [56937855b121fede4013141a6cf4ca46] A log message goes here.
</code></pre>

<p>We can eyeball our log file then to see the logs grouped together. Or, we could build a set of little shell commands to do that for us:</p>

<pre><code>
reqs() {
  req_id=$(tail -n 1000 log/development.log |
    awk -F'[][]' '{print $2, "|", $4}' | sort -u -r | fzf | awk '{print $NF}')

  reqlogs "$req_id"
}

reqlogs() {
  awk -v req_id="$1" '
  $0 ~ "\\[" req_id "\\]" {
    sub(/\[[0-9a-f]+\]/, "", $0)
    print
  }
  ' log/development.log
}
</code></pre>

<p>The <code>reqs</code> command here uses <code>awk</code> and <a href="https://github.com/junegunn/fzf">fzf</a> to find the last 1,000 log lines, and outputs the timestamps and request IDs for them, with the most recent request selected by default:</p>

<pre><code>2025-02-02T16:32:35.6772+11:00 | 56937855b121fede4013141a6cf4ca46
&gt; 2025-02-02T16:34:36.1173+11:00 | 3a87e274ee9f81c898d9d85abb0a8dd2
</code></pre>

<p>Once one is selected, it then uses the <code>reqlogs</code> function to display just the log messages that match that ID. Given we already know what the ID is, there’s no need to display it so <code>reqlogs</code> snips that bit out as that’ll save 32 characters each time.</p>

<p>What we’ll end up with here is a set of log lines that match only <em>one</em> request at a time:</p>

<pre><code>[2025-02-02T16:32:35.6772+11:00] A log message goes here.
</code></pre>

<p>This is much nicer than trawling through a giant log file or scrolling back through my console to find the particular lines I’m after!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A feature within Rails is that it allows us to add data to our application’s log lines for tagging those lines. We can then use these tags for aggregating them together into a bunch. I’ll show you how to do this here. I used this in a Rails app that acts purely as an API, so there is only ever one request at a time I care about, in this case.]]></summary></entry><entry><title type="html">Scoping an Active Record join</title><link href="http://localhost:4000/2024/12/scoping-an-active-record-join" rel="alternate" type="text/html" title="Scoping an Active Record join" /><published>2024-12-09T00:00:00+11:00</published><updated>2024-12-09T00:00:00+11:00</updated><id>http://localhost:4000/2024/12/scoping-an-active-record-join</id><content type="html" xml:base="http://localhost:4000/2024/12/scoping-an-active-record-join"><![CDATA[<p>Active Record is well known for its footguns, like N+1 queries and letting you dump <em>all the business logic</em> for your applications in models. (For an alternative, read <a href="https://leanpub.com/maintain-rails">Maintainable Rails</a>.)</p>

<p>A lesser-known footgun is this seemingly innocuous use of <code>joins</code> in a tenanted Rails application. By “tenanted” I mean that most records have something like a <code>tenant_id</code> on them that declares ownership. In our case, it’s <code>merchant_id</code>. Here’s the query:</p>

<pre><code class="language-ruby">FraudCheck.where(merchant: merchant).joins(:purchase)
</code></pre>

<p>Fraud checks belong to a merchant, and they also belong to a purchase. Purchases have just the one fraud check. Merchants have many fraud checks and purchases.</p>

<p>The query this executes is:</p>

<pre><code class="language-sql">SELECT "fraud_checks".* FROM "fraud_checks"
INNER JOIN "purchases" ON "purchases"."id" = "fraud_checks"."purchase_id"
WHERE "fraud_checks"."merchant_id" = 1
</code></pre>

<p>This seems like a relatively good query and it’ll run “fast enough” on small data sets. However, as your dataset grows and becomes measured in multiple terabytes, such a query will get slower and slower.</p>

<p>This query runs slow because it’s querying two tables, one very quickly because it has a small dataset to query through, and one very slowly because it has a much larger dataset to trawl through. The first table it queries is <code>fraud_checks</code>, and it finds all of those where the <code>merchant_id=1</code>, which is a smaller dataset than “all fraud checks ever”. The second table it queries is “purchases”, which it attempts to find all purchases from all time matching the <code>purchase_id</code> values returned by the fraud checks query.</p>

<p>We can shorten this query’s execution time by scoping the purchases to just those from the merchant by using <code>merge</code>:</p>

<pre><code class="language-ruby">FraudCheck
  .where(merchant: merchant)
  .joins(:purchase)
  .merge(
    Purchase.where(merchant: merchant)
  )
</code></pre>

<p>This now executes this query:</p>

<pre><code class="language-sql">SELECT "fraud_checks".* FROM "fraud_checks"
INNER JOIN "purchases" ON "purchases"."id" = "fraud_checks"."transaction_id"
WHERE "fraud_checks"."merchant_id" = 2736
AND "purchases"."merchant_id" = 2736
</code></pre>

<p>The query is now limited to just fraud checks <em>and</em> purchases that match that <code>merchant_id</code>, resulting in a smaller table scan for purchases that match the selected fraud checks.</p>

<p>We further limit this query by applying a date range scope on the purchases too:</p>

<pre><code class="language-ruby">FraudCheck
  .where(merchant: merchant)
  .joins(:purchase)
  .merge(
    Purchase.where(merchant: merchant, created_at: start_date..end_date)
  )
</code></pre>

<p>This results in a super fast query compared to what we started with, as we’ve now drastically reduced the scope of purchases that can match our query.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Active Record is well known for its footguns, like N+1 queries and letting you dump all the business logic for your applications in models. (For an alternative, read Maintainable Rails.)]]></summary></entry><entry><title type="html">Using Elastic Search’s Profile API</title><link href="http://localhost:4000/2024/12/using-elastic-searchs-profile-api" rel="alternate" type="text/html" title="Using Elastic Search’s Profile API" /><published>2024-12-05T00:00:00+11:00</published><updated>2024-12-05T00:00:00+11:00</updated><id>http://localhost:4000/2024/12/using-elastic-searchs-profile-api</id><content type="html" xml:base="http://localhost:4000/2024/12/using-elastic-searchs-profile-api"><![CDATA[<p>Recently, we saw that one of our production applications was experiencing very long query times when users were searching for their customers, with some queries taking as long as 60 seconds.</p>

<p>We use Elastic Search to power this search (even though Postgres’ own full-text search would’ve been suitable!) and the query we wrote for Elastic Search was this one written about 10 years ago:</p>

<pre><code class="language-json">{
  "query": {
    "bool": {
      "must": [
        {
          "query_string": {
            "query": "Ryan*"
          }
        }
      ],
      "filter": [
        {
          "bool": {
            "must": [
              {
                "terms": {
                  "merchant_id": [2]
                }
              }
            ]
          }
        }
      ]
    }
  }
}
</code></pre>

<p>This query will search for the query string “Ryan*” across all fields on all documents within the <code>customers</code> index. Given the application has grown substantially over the last 10 years, there’s now <em>a lot</em> of customer documents to search through. As the number of documents grow, the amount of time to search through those documents gets increasingly slower.</p>

<p>In order to figure out <em>why</em> this query was slow rather than “big data” and vibes-driven-development, I turned to the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-profile.html">Profile API within Elastic Search</a>. We can use this by adding <code>profile: true</code> to the top of any query string:</p>

<pre><code class="language-json">{
  "profile": true,
  "query": {
    "bool": {
  ...
</code></pre>

<p>This profile key gives us a <em>very</em> detailed breakdown of what a query is doing, including how long each of its distinct parts are taking. Fortunately for us, this query is relatively “simple” and only consists of one very large operation: search across all document fields for a wildcarded query string.</p>

<p>The first thing I noticed when looking at this output is that the number of fields are quite long:</p>

<pre><code>{
  "profile": {
    "shards": [
      {
        "id": "[JzYnfX2ORHiGumsVoL3jhg][customers][2]",
        "searches": [
          {
            "query": [
              {
                "type": "BooleanQuery",
                "description": "(last_name.keyword:Ryan* | &lt;a lot of fields&gt;"
              }
            ]
          }
        ]
      }
    ]
  }
}
</code></pre>

<p>The excessive amount of fields are a combination of regular customer information and address information. So my first thought was could we limit the amount of fields that we’re letting users search through. To do this, we can use <code>fields</code> on the query to say “only search these fields”:</p>

<pre><code class="language-json">{
  "profile": true,
  "query": {
    "bool": {
      "must": [
        {
          "query_string": {
            "query": "Ryan*",
            "fields": [
              "first_name",
              "last_name",
              "email",
              "reference",
              "card_token",
              "card_number",
              "public_id"
            ]
          }
        }
      ],
      "filter": [
        {
          "bool": {
            "must": [
              {
                "terms": {
                  "merchant_id": [2]
                }
              }
            ]
          }
        }
      ]
    }
  }
}
</code></pre>

<p>This time the profile output only contained the fields that I was interested in. These fields are all the fields we display in the UI for customers – notably <code>card_number</code> is a masked version of the number.</p>

<p>After making this change, the query time went from multiple-digit seconds to single-digit seconds. This is because the query now looks in fewer locations across each document within its index. Importantly, the query also passed all our feature tests around searching within our application too.</p>

<p>I still felt like there was space to improve the query. Did we really need it to use a wildcard search, given that Elastic Search is pretty decent at matching text? So I tried it again without the wildcard on the end of the query:</p>

<pre><code class="language-json">{
  "profile": true,
  "query": {
    "bool": {
      "must": [
        {
          "query_string": {
            "query": "Ryan",
            "fields": [
              "first_name",
              "last_name",
              "email",
              "reference",
              "card_token",
              "card_number",
              "public_id"
            ]
          }
        }
      ],
      "filter": [
        {
          "bool": {
            "must": [
              {
                "terms": {
                  "merchant_id": [2]
                }
              }
            ]
          }
        }
      ]
    }
  }
}
</code></pre>

<p>This query now operated in two-digit milliseconds. Without using a wildcard, the query is pre-analysed by Elastic Search and breaks it down into tokens that can then be matched to pre-analysed documents within the index.</p>

<p>Comparing the two profile outputs, the one with the wildcard shows a series of <code>MultiTermQueryConstantScoreWrapper</code>, matching against all different fields. The one without the wildcard shows a range of different ones such as <code>TermQuery</code> for fields classified as <code>term</code>, which will match faster as we’re searching based on the pre-analysed data within the index.</p>

<p>(And if we want to be completely un-scientific about it, the profile output for wildcard searching is 1,100 lines, while the profile output for non-wildcard searching is only 700 lines. Fewer lines of profiling output is a very good indicator that the searcher is doing less work!)</p>

<p>This is more suitable for matching against customer records in most circumstances, as our users are searching either by a customer’s full name or their email addresses. In rarer cases, they’re using reference values, and when that happens it appears to be the full reference value. The <code>card_token</code> and <code>card_number</code> fields are used the least frequently.</p>

<p>I’m going to be rolling out this change next week and I have strong faith in its ability to reduce search time for people. I now have an additional tool in my toolbelt for diagnosing slow Elastic Search queries, and a better understanding from the profile output as to what different queries are doing.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Recently, we saw that one of our production applications was experiencing very long query times when users were searching for their customers, with some queries taking as long as 60 seconds.]]></summary></entry><entry><title type="html">React is fine</title><link href="http://localhost:4000/2024/11/react-is-fine" rel="alternate" type="text/html" title="React is fine" /><published>2024-11-26T00:00:00+11:00</published><updated>2024-11-26T00:00:00+11:00</updated><id>http://localhost:4000/2024/11/react-is-fine</id><content type="html" xml:base="http://localhost:4000/2024/11/react-is-fine"><![CDATA[<p><a href="https://joshcollinsworth.com/blog/antiquated-react">This post called “Things you forgot (or never knew) because of React</a> by Josh Collinsworth is a good read about the deficiencies of React, and includes a good comparison between React and the other frontend frameworks at the moment.</p>

<p>And yet, I find myself and my team consistently being productive with React. The main application we develop uses a lot of it, a second application has had a re-write of a key component into React, and other apps have “React sprinkles” through them. It’s a versatile framework!</p>

<p>In our main application, we have React componentry brought in from our design system, which is then bundled together into much larger components. Most of these are static components: take some data, render an element a certain way depending on that data. Where we use React’s “reactivity” is typically in a few small places:</p>

<ol>
  <li>Make this menu appear when its “open” button is clicked</li>
  <li>Show a loading spinner while a request is processing</li>
  <li>Display a validation error message if a field doesn’t pass validation (for example: a card expiry that is in the past, or an invalid card number – neither of which browsers support natively.)</li>
</ol>

<p>We also leverage a lot of what GraphQL provides by exporting types from the backend to then inform types on the frontend. Yes, we <em>could</em> do this <a href="https://the-guild.dev/graphql/codegen/docs/guides/svelte">with another framework</a> but even adding a single component that uses this framework doubles our team’s cognitive load for what seems like minimal benefit. These GraphQL types then go on to inform what the data used in those React components of the app should look like.</p>

<p>In terms of styling: we use Tailwind, which I covered in <a href="https://ryanbigg.com/2024/03/tailwind-has-won">“Tailwind has won”</a>. We don’t need styles that are limited in scope to a particular component because of how Tailwind operates – it’s all utility classes and they don’t apply <em>until you apply them</em>. Yes, you can have really really long class lists, but you can compress these down into your own utility classes, as we’ve done with things such as <code>.zeal-button-primary</code>.</p>

<p>Two things that we don’t have yet in how our applications operate are server-side rendering and web components.</p>

<p>Server-side-rendering would mean that we could get away with displaying dynamic data, still using our existing React components, without displaying loading spinners all over the place. It’s a trivial thing, but a loading spinner makes me think “this app could’ve taken an extra 100ms to fetch this data in the original request”. We could probably get there with a little effort, though I do wonder how it’d work with the GraphQL things we have in place.</p>

<p>On web components: I would like to move parts of our design system towards adopting those. I’m somewhat wary of the “newness” of interactivity between React + web components, and also about the “split brain” of switching between “this is a React component” and “this is a web component”. But I think web components is ultimately where we’re headed, as the browser always wins.</p>

<p>(And on one more note: Don’t get me started on Stimulus.)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This post called “Things you forgot (or never knew) because of React by Josh Collinsworth is a good read about the deficiencies of React, and includes a good comparison between React and the other frontend frameworks at the moment.]]></summary></entry><entry><title type="html">Ruby Retreat 2024</title><link href="http://localhost:4000/2024/10/ruby-retreat-2024" rel="alternate" type="text/html" title="Ruby Retreat 2024" /><published>2024-10-22T00:00:00+11:00</published><updated>2024-10-22T00:00:00+11:00</updated><id>http://localhost:4000/2024/10/ruby-retreat-2024</id><content type="html" xml:base="http://localhost:4000/2024/10/ruby-retreat-2024"><![CDATA[<p>This weekend was Ruby Retreat (a re-branded <a href="https://rails.camp/">Rails Camp</a>) where we gathered 60 people from Australia and New Zealand at a camp ground in Warrnambool, the town where I live. Ruby Retreat is an un-conference event where Rubyists of all skill levels come together to hang out from a Friday night until a Monday morning. There may have even been some non-Rubyists there too. We provided catering and beds, they provided the activities.</p>

<p>The idea for this event came out of a Ruby Australia conference earlier in the year when a group of Ruby friends pulled me aside and said “we should have a camp again!”. We’ve had about 27 of these in the past, with them dating back to 2007. Covid threw a spanner in the works and we ended up not running one for a while.</p>

<p>After a few suggestions of locations, I suggested Warrnambool, pitching that there’s a venue there that’s close to the beach and there’s plenty of activities near by. It sounded enough like a good idea that I was suddenly made de facto organiser of the camp. Others such as Jupiter Haehn, Kieran Andrews, Ed Tippett and Richie Khoo helped out too and offered very good advice.</p>

<h2 id="selling-tickets">Selling tickets</h2>

<p>We sold tickets to the camp by advertising them at https://retreat.ruby.org.au by recycling a previous version of the website, and selling them through <a href="https://ti.to">Tito</a>. We eye-balled a rough estimate on what the camp would cost us and used that to set the ticket prices ($350 full price, $300 concession). We weren’t too far off, with the debt (measured in &lt; $2,000 terms) being covered by Ruby Australia’s sponsors for the year.</p>

<p>I think we could’ve done a better job with marketing the camp, probably by having organisers (or proxies) visit each meetup around the country and spruik the benefit of it.</p>

<p>We ended up selling 60 tickets to the event and also gave an option for people to put in for an Opportunity Ticket cost. This was enough to bring one extra person <em>for free</em> along to the camp. We didn’t make a big deal about it at the camp but I reckon it is a big deal! Generosity like this is awesome to see from this community.</p>

<h2 id="location-location-location">Location, Location, Location</h2>

<p>The camp site was Warra Gnan Coastal Camp, located 600m from the ocean. Liasing with the camp site owners was a relatively straightforward affair with tours given early on in the planning process. Big “ticks” for why we picked that place (besides it being 3km from my house) was the location, the kitchen area and the sufficient beds down the back area. Some of the rooms contained en suites with toilets and showers, while the others had a pair of shower rooms, one for men and one for women.</p>

<p>Other perks included the grassed area at the back for tent setup (some people like to camp away from the snorers!), and ample outside room for people who wanted to catch some sun during the day.</p>

<p>Closer towards the camp the camp site also installed a projector in the main space as well as two heaters. We made ample use of this projector for talks at beginning and end of camp. We didn’t end up needing the heaters but in the colder months they’d be a necessity.</p>

<p>The location also meant people could get to Thunder Point, Stingray Bay, the main beach, Lake Pertobe and the Sunday markets by walking. Having it so close to town as well meant supplies could be easily gathered if people needed anything that the camp didn’t have. Warrnambool’s a big enough town that there’s multiple Coles, Woolworths and Aldis.</p>

<h2 id="catering">Catering</h2>

<p>Catering was provided by the Beach Kiosk Cafe. After being inflicted with Tinned Spaghetti Bolognese In A Big Pot and Some Toast with Spreads masquerading as dinner and/or lunch at a long-ago past event, I wanted something better for catering options this time around. I reached out to the Beach Kiosk who I know through a family-connection and they were happy to provide the catering. We catered for lunch and dinners with this menu:</p>

<blockquote>
  <p>DF = Dairy Free, GF = Gluten Free, V = Vegetarian, VG = Vegan</p>

  <p><strong>Dinner 18th October</strong></p>

  <p>Slow braised lamb shoulder in with tomato paprika sauce(DF GF)</p>

  <p>Grilled chicken with onions, capsicum, and olives(DF GF)</p>

  <p>Roasted half eggplant w tomato and paprika(VG)</p>

  <p>Ratatouille(VG GF DF)</p>

  <p>Green beans, feta and pine nuts(GF V)</p>

  <p>Garlic mash potato(GF)</p>

  <p>Banana fritter with vanilla ice cream
Sticky rice w cinnamon coconut sauce with banana and berries(VG DF GF)</p>

  <p><strong>Lunch Oct 19</strong></p>

  <p>Stir fry egg noodles with chickened veggies(DF VGA)</p>

  <p>Pork belly fried rice(GF DF VGA)</p>

  <p>Fruit Salad</p>

  <p><strong>Dinner Oct 19</strong></p>

  <p>Ginger soy braised beef with shitake mushroom (DF GF)</p>

  <p>Crispy tofu with miso chilli with cauliflower, mushroom and herbs (VG GF DF)</p>

  <p>Asian Greens w soy and garlic(VG)</p>

  <p>Fragrant rice(VG DF GF)</p>

  <p>Strawberry panacotta with coconut chocolate mousse and fresh berries (VG DF GF)</p>

  <p><strong>Lunch Oct 20</strong></p>

  <p>Roast pumpkin, roast cauliflower, mushroom, beetroot hummus, haloumi, seeds, and cashew aioli (VG GF DF)</p>

  <p>Fruit Salad</p>

  <p><strong>Dinner Oct 20</strong></p>

  <p>Roast cauliflower and fennel paella (VG GF DF)</p>

  <p>Chicken chorizo paella(GF DF)</p>

  <p>Salad with mix lettuce, carrots, red onion, cucumber and fennel, with vinaigrette dressing(VG DF GF)</p>

  <p>Raspberry Cheesecake(GF)</p>

</blockquote>

<p>You’ll notice that most meals were suitable for vegans or vegetarians. This is so that we could include as many people as possible for lunches and dinners. This food worked out to $60/head/day. It was exceptional each night and as someone who has been called “food-oriented” on more than one occasion, I really appreciated the quality and good mix of healthy veg and meat. And desserts! Other camp attendees rated the food highly too!</p>

<p>On top of this, we also bought breakfast cereals, milk, coffee, bread and spreads so people could build their own breakfasts. Jupiter also bought an absolute wealth of snacks from Costco for cheap. Kieran cooked trays of scrambled eggs on the morning. On the Sunday and Monday mornings we also made a tray of pancakes for early risers. Next time I’d bring a second pan so I could cook them faster!</p>

<p>All 3 mornings had the coffee van turn up for 2 hours (8-10 Saturday/Sunday, 7-9 Monday) where people could order their coffees and we covered the cost of those too.</p>

<p>I was arranging the washing up on Friday night and outta nowhere an attendee, Michael Morris, self-organised a schedule for the rest of the camp, taping a laminated piece of paper to a wall with a whiteboard-marker schedule on it. I damn near cried it was that nice of an offer. The cleaning up on Saturday/Sunday/Monday was a lot easier than Friday to the point where I didn’t need to think about it for the rest of the camp.</p>

<p>In terms of drinks, we catered by buying some juices and milk, and others brought their own soft drinks or alcohol and put them into the shared fridge. Rails Camps in the past have catered for their own alcohol but we decided not to this time. We spent the alcohol money on kick-ass food instead. Those that wanted to drink could still do it, just on their own dollar. Nobody complained.</p>

<h2 id="transport">Transport</h2>

<p>We had a few people fly in from New Zealand and one guy flew in from Japan (!), but most people came in from around Australia and ended up arriving in Melbourne. Warrnambool has a <em>private</em> airport, so there’s no direct commercial flights in.</p>

<p>Some of those people drove over from Melbourne (despite the stormy conditions on Friday afternoon). The remaining group of around 20 caught the train over, with the camp funding the $20/ticket/day cost for those tickets. We had Brent Chuang from Fat Zebra being the “ticket holder” for the train tickets as he was catching the train from Melbourne. I didn’t mind this too much, but I would’ve preferred V/Line offered an electronic option that was easier to manage than paper tickets.</p>

<p>As the camp approached it became clearer and clearer that the weather on Friday was going to be very bad, so we ended up booking a bus at $10/head for that day. And glad we did, as the weather was stormy all day. Attendees who caught the bus ended up being treated to a (surprise) mini-tour of Warrnambool too thanks to the bus tour company. They arrived dry at the camp, which is always a nice way to get a start to an event.</p>

<p>There was no bus back on the Monday afternoon as the time of venue departure was 9am and the train was due to depart at 12pm, and the weather was <em>exceptionally sunny</em>. This left people able to explore Warrnambool more and find their own way to the station.</p>

<h2 id="inside-the-building-itself">Inside the building itself</h2>

<p>Internet: This time there was no internet access at the camp <em>by choice</em> rather than from past camps where it’s absolutely an <em>impracticality</em> due to location (i.e. the last Ruby Retreat, held up the top of a mountain in New Zealand). I ummed and ahhed about setting up a proper router with a 5G sim in it from Telstra but ultimately decided people could sort out their own access with their hotspots on their phones. It seemed to work alright. I think with a shared router between 50 people you’d run into people/machines being greedy, or random network issues like “this router has handed out 32 DHCP addresses and refuses to do anymore”. Perhaps we dodged a bullet.</p>

<p>Tables &amp; powerboards: Some of the attendees setup the tables and powerboards with minimal direction (thank you!!). We found that we were mildly <em>short</em> of powerboards for the number of people, but ultimately other people ended up bringing their own and making up the numbers. There was just enough power sockets in the walls for these and we ran out extension cables from these sockets to power the boards.</p>

<p>We were at capacity for the tables at the camp as well (but strangely, not the provided chairs…), and perhaps if we were to use this venue again we’d have to hire some more tables. Fortunately, I found an events hire company that would do that for a reasonable rate of $18/table. We didn’t end up needing them this time, but I imagine with higher attendance it’d be high on the list of stuff to organise. There’s room in the venue for more. Not only was this where people sat and worked, but it’s also where they had dinner. We also had tables outside that people made use of for this too.</p>

<p>I couldn’t imagine having an attendance above about 75 adults at the camp because it’d be super cozy in the main hall.</p>

<h2 id="activities">Activities</h2>

<p>As it’s an unconference we’re extremely loosey-goosey when it comes to actually scheduling anything in. We had the welcome on the Friday night, a Ruby Australia AGM on Saturday afternoon, and a demo night on Sunday night. That’s it. If anyone else wanted to do anything else, they had to make up their own plans. A lot of people spent the weekend hacking on things, and a similar number spent the time just hanging out. You make the event how you want to make it.</p>

<p>I ended up playing Magic for a few hours on Saturday against Kieran and whoever else wanted to join us. I also extended my <a href="https://github.com/radar/mtg">Magic project</a> with a few new cards.</p>

<p>Jupiter ran a few sessions on Blood on the Clocktower, which is more than a spiritual successor to the traditional Werewolf. That was really fun to play! I love the diversity of the roles (instead of a few sporadic “specials”), the interplay between alive &amp; dead people, and the mechanics of the imp + minion. Definitely recommend!</p>

<p>A group formed around a table-top game like DND but not DND (I didn’t catch the name!) on Sunday afternoon and played that long enough for the guy who was running it to turn a great shade of red from his sunburn.</p>

<p>Other attendees ended up touring around Warrnambool doing things like riding the flying fox at Lake Pertobe, walking around the coastal walk at Thunder Point or visiting the nearby beaches. Some people were even able to make bookings at the Deep Blue Hot Springs. Others spent time further afield going out to Tower Hill and other locations.</p>

<h2 id="would-i-do-it-again">Would I do it again?</h2>

<p>Yes. This was really fun to organise and be a part of. I don’t think I’d run one again in the next 6 months, but perhaps in a year? We’ll see.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This weekend was Ruby Retreat (a re-branded Rails Camp) where we gathered 60 people from Australia and New Zealand at a camp ground in Warrnambool, the town where I live. Ruby Retreat is an un-conference event where Rubyists of all skill levels come together to hang out from a Friday night until a Monday morning. There may have even been some non-Rubyists there too. We provided catering and beds, they provided the activities.]]></summary></entry></feed>